# -*- coding: utf-8 -*-
"""weights.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12UUcVaVohb11NAnnbuBj6lvZJMmWauBR

# Feature Importances for Water Quality Analysis

This notebook explains the process of calculating feature importances for a water quality classification model.
The analysis is conducted using advanced techniques like SHAP (SHapley Additive exPlanations) values, which provide insights into the contributions of individual features.

## Steps Covered:
1. Loading the trained model and dataset
2. Preparing the data for interpretation
3. Using SHAP to calculate feature importance
"""

import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import shap

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation

data = pd.read_csv('/content/training_data.csv')
print(data.head())

X = data.drop('Health_Status', axis=1)
y = data['Health_Status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)

print(X_train.shape)
print(X_val.shape)
print(X_test.shape)
print(y_train.shape)
print(y_val.shape)
print(y_test.shape)

model = Sequential([
    Dense(64, input_shape=(9,), activation='relu'),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(8, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy' ,
              metrics=['accuracy'])

history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]
)

test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

background_size = 100
random_indices = np.random.choice(X_train.shape[0], background_size, replace=False)
background = X_train.iloc[random_indices].values

# Initialize the DeepExplainer
explainer = shap.DeepExplainer(model, background)

X_test_np = X_test.values

# Compute SHAP values for X_test
shap_values = explainer.shap_values(X_test_np)

print("SHAP values shape:", shap_values.shape)  # Expected: (num_samples, num_features)

print(shap_values[0])

avg_shap = np.mean(np.abs(shap_values), axis=0)

print(avg_shap)

normalized_relevance= avg_shap / np.sum(avg_shap)
print(normalized_relevance)

lrp_val = [-0.15988433,  0.3479544,   0.04085304,  0.01621986, -0.04715924, -0.03599263,
 -0.04263536,  0.02462474,  0.06745085]

lrp_val_abs = np.abs(lrp_val)
normalized_lrp = lrp_val_abs / np.sum(lrp_val_abs)
print(normalized_lrp)